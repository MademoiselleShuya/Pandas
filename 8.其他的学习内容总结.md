这部分的内容作为了解，之后如果用到，再返回来查看，重新学习

学习内容

**pandas批量拆分于和合并excel文件**

1. 将一个大的excel拆分成多个excel

```
工作逻辑：
计算拆分后的每一个excel的行数
拆分成多个dataframe，用到iloc，append
将每个dataframe存入excel，用df.to_excel()
```

2. 将多个小的excel合并成一个大excel并标记来源

```
工作逻辑：
遍历文件夹，得到要合并的excel文件列表(创建一个列表，放置excel文件名，添加文件名append)
分别读取到dataframe，每个df添加一列用于标记来源(创建一个列表，存放小的dataframe，用append添加，修改名称可以使用字符串的切片语法)
使用pd.concat进行df批量合并
将合并的dataframe输入到excel
```

**pandas怎样实现groupby分组数据统计**

类似SQL：

Select city, max(temperature) from city_weather group by city

groupby：先对数据分组，然后再每个分组上应用聚合函数，转换函数

1. 分组使用聚合函数做数据统计

```
单个列groupby，查询所有数据列的统计，这个过程会用到pandas，numpy，以及%matplotlib inline
dataFrame_Iwant.groupby("列名字").sum()
groupby中的选中的列变成了数据的索引列；同时统计sum，会将不是数字的列排除在外，自动忽略

多个列groupby，查询所有数据列的统计
dataFrame_Iwant.groupby("列名字1", "列名字2").mean()
此时"列名字1"，"列名字2"，变成了二级索引，如果设置参数为as_index=False就不会变成索引，而仍为普通的列

同时查看多种数据统计
dataFrame_Iwant.groupby("列名字").agg([np.sum, np.mean, np.std])
这时候列索引会变成多级索引

查看单个列的结果数据统计
dataFrame_Iwant.groupby("列名字1")["列名字2"].agg([np.sum, np.mean, np.std])
在groupby之后再筛选出"列名字2"列的数据统计，这个时候就没有多级索引了
也可以写成dataFrame_Iwant.groupby("列名字1").agg([np.sum, np.mean, np.std])["列名字2"]

不同列使用不同的聚合函数
dataFrame_Iwant.groupby("列名字1").agg({"列名字2":np.sum, "列名字3":np.mean})
```

2. 遍历groupby的结果理解执行流程

```
for循环可以直接遍历每个group
```

**pandas的分层索引multiIndex的使用**

分层索引：在一个轴上拥有多个索引层级，可以表达更高维度数据的形式

可以方便进行数据筛选，如果有序性能更好

groupby等操作的结果，如果是多KEY，结果是分层索引，需要会使用

一般不需要自己创建分层索引（MultiIndex有构造函数但一般不用）

```
Series的分层索引MultiIndex
Series_Iwant.groupby(["A", "B"])["C"].mean()
多层索引中空白是指使用上面的值
ser = Series_Iwant.groupby(["A", "B"])["C"].mean()
ser.index
ser.unstack() 把多级索引变成列，相当于ser.reset_index()

Series有多层索引怎么样筛选数据
ser.loc["一级索引名字"]
ser.loc["一级索引名字", "二级索引名字"]
ser.loc[:, "二级索引名字"]

DataFrame的多层索引MultiIndex
DataFrame_Iwant.set_index(["索引名字1", "索引名字2"], inplace=True)

DataFrame有多层索引怎么样筛选数据
在传入数据过程中需要注意：
元组(key1, key2)代表筛选多层索引，其中key1是一级索引，key2是第二级
列表[key1, key2]代表同一层索引的多个key，key1和key2是并列的同级索引
DataFrame_Iwant.loc[(slice(None), ["key2.1", "key2.2"])]
(slice(None)代表筛选这一索引的所有内容
DataFrame_Iwant.reset_index()
```

**pandas的转换函数map、apply、applymap**

数据转换函数对比：map、apply、applymap

1. map只用于Series，实现每个值>值的映射
2. apply用Series实现每个值的处理，用于Dataframe实现某个轴的Series的处理
3. applymap：只能用于Dataframe，用于处理该Dataframe的每个元素

```
map用于Series值的转换
Series.map(dict) or Series.map(function)均可

apply用于Series和DataFrame.的转换
Series.apply(function)，函数的参数是每一个值
DataFrame.apply(function)，函数的参数是Series

applymap用于DataFrame所有值的转换
DataFrame.applymap(function)
```

**pandas对每个分组应用apply函数**

pandas的groupby遵从split、apply、combine模式

这里的split是指pandas的groupby，我们自己实现apply函数，apply返回的结果由pandas进行combine得到结果

GroupBy.apply(function)

function的第一个参数是dataframe

function的返回结果，可是dataframe、series、单个值、甚至和输入dataframe完全没关系

```
实现对数值列按分组归一化
将不同范围的数值进行归一化，映射到[0,1]区间，好处是更容易做数据的横向对比，以及机器学习模型会学的更快效果和性能也可能更好
apply(归一化公式)

取每个分组的TOPN数据
apply(取得TOPN的函数，参数)
```

**pandas使用stack和pivot实现数据透视**

将列式数据变成二维交叉形式，便于分析，叫做重塑或透视

```
经过统计得到多维指标数据

使用unstack实现数据二维透视
DataFrame_Iwant_group.unstack()
unstack和stack是互逆操作
df_group_stack.plot
unstack将index转换成columns，stack反之

使用pivot简化透视
pivot相当于对DataFrame使用set_index创建分层索引，然后调用unstack

stack、unstack、pivot的语法
stack: DataFrame.stack(level=-1, dropna=True)，将columns变成index，类似把横放的书籍变成竖放
level=-1，代表多层索引的最内层，可以通过==0、1、2指定多层索引的对应层
unstack：DataFrame.unstack(level=-1, fill_value=None)把index变成columns，相当于把竖放的书籍横放
pivot：DataFrame.pivot(index=None, columns=None, values=None)，指定index、columns、values实现二维透视
```

**pandas使用apply函数给表格添加多列**

```python
给表格题添加一列：

def add_one_column(row)
	new_column = row["a"] + ["b"]
	return new_column

df["new_column"]=df.apply(add_one_colum, axis=1)


给表格添加多列

def add_columns(row)
	new_column1, new_column2 = row["a"] + ["b"], row["a"] - ["b"]
	return new_column1, new_column2

df[["new_column1", "new_column2"]]=df.apply(add_columns, axis=1, result_type="expand")

```



学吐🤮了，这两天不想碰pandas了！😭😭